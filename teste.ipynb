{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fichier                 | Précision | Total 'that' | Correctement annotés |\n",
      "|-------------------------|----------|-------------|---------------------|\n",
      "| that_adv.txt            | 5.00% | 100          | 5                    |\n",
      "| that_conjunction.txt    | 99.00% | 100          | 99                   |\n",
      "| that_determiner.txt     | 86.00% | 100          | 86                   |\n",
      "| that_pronoun.txt        | 19.00% | 100          | 19                   |\n",
      "\n",
      "Les annotations sont sauvegardées dans les fichiers correspondants.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import treetaggerwrapper\n",
    "\n",
    "# Définir les chemins\n",
    "tagger_dir = os.path.expanduser(\"~/treetagger\")  # Dossier TreeTagger\n",
    "param_file = os.path.expanduser(\"~/treetagger/english.par\")  # Modèle anglais\n",
    "\n",
    "# Dictionnaire des fichiers de test et des étiquettes attendues\n",
    "test_files = {\n",
    "    \"that_adv.txt\": \"RB\",\n",
    "    \"that_conjunction.txt\": [\"IN/that\", \"CST\", \"CJT\"],\n",
    "    \"that_determiner.txt\": \"DT\",\n",
    "    \"that_pronoun.txt\": \"WDT\"\n",
    "}\n",
    "\n",
    "# Initialiser TreeTagger\n",
    "assert os.path.exists(param_file), \"Le fichier de paramètres n'existe pas.\"\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGDIR=tagger_dir, TAGPARFILE=param_file)\n",
    "\n",
    "# Résultats globaux\n",
    "results = {}\n",
    "\n",
    "# Boucle sur chaque fichier de test\n",
    "for file_name, expected_tags in test_files.items():\n",
    "    input_file = os.path.expanduser(f\"~/treetagger/data/{file_name}\")  # Fichier d'entrée\n",
    "    output_file = os.path.expanduser(f\"~/treetagger/data/{file_name.replace('.txt', '-annotation.txt')}\")  # Fichier de sortie\n",
    "\n",
    "    assert os.path.exists(input_file), f\"Le fichier {input_file} n'existe pas.\"\n",
    "\n",
    "    # Lire le fichier de test\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().splitlines()\n",
    "\n",
    "    # Ouvrir le fichier de sortie\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        total_that = 0\n",
    "        correct_that = 0\n",
    "\n",
    "        # Appliquer TreeTagger et enregistrer les annotations\n",
    "        for line in text:\n",
    "            words = line.strip().split()\n",
    "            if words:\n",
    "                tags = tagger.tag_text(line)\n",
    "                for tagged_word in tags:\n",
    "                    parts = tagged_word.split(\"\\t\")\n",
    "                    if len(parts) == 3:\n",
    "                        word, predicted_tag, lemma = parts\n",
    "                        # Sauvegarder dans le fichier\n",
    "                        out_f.write(f\"{word}\\t{predicted_tag}\\t{lemma}\\n\")\n",
    "\n",
    "                        # Vérifier si \"that\" est bien annoté\n",
    "                        if word.lower() == \"that\":\n",
    "                            total_that += 1\n",
    "                            if isinstance(expected_tags, list):  # Si plusieurs tags sont possibles\n",
    "                                if predicted_tag in expected_tags:\n",
    "                                    correct_that += 1\n",
    "                            else:\n",
    "                                if predicted_tag == expected_tags:\n",
    "                                    correct_that += 1\n",
    "\n",
    "    # Calculer la précision sur \"that\"\n",
    "    accuracy = correct_that / total_that if total_that > 0 else 0\n",
    "    results[file_name] = {\"total_that\": total_that, \"correct\": correct_that, \"accuracy\": accuracy}\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"| Fichier                 | Précision | Total 'that' | Correctement annotés |\")\n",
    "print(\"|-------------------------|----------|-------------|---------------------|\")\n",
    "for file_name, data in results.items():\n",
    "    print(f\"| {file_name:<23} | {data['accuracy']:.2%} | {data['total_that']:<12} | {data['correct']:<20} |\")\n",
    "\n",
    "print(\"\\nLes annotations sont sauvegardées dans les fichiers correspondants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fichier                 | Précision | Total 'that' | Correctement annotés |\n",
      "|-------------------------|----------|-------------|---------------------|\n",
      "| that_adv.txt            | 0.00% | 100          | 0                    |\n",
      "| that_conjunction.txt    | 96.00% | 100          | 96                   |\n",
      "| that_determiner.txt     | 91.00% | 100          | 91                   |\n",
      "| that_pronoun.txt        | 97.00% | 100          | 97                   |\n",
      "\n",
      "Les annotations sont sauvegardées dans les fichiers correspondants.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import treetaggerwrapper\n",
    "\n",
    "# Définir les chemins\n",
    "tagger_dir = os.path.expanduser(\"~/treetagger\")  # Dossier TreeTagger\n",
    "param_file = os.path.expanduser(\"~/treetagger/english-bnc.par\")  # Modèle anglais\n",
    "\n",
    "# Dictionnaire des fichiers de test et des étiquettes attendues\n",
    "test_files = {\n",
    "    \"that_adv.txt\":  \"RB\",\n",
    "    \"that_conjunction.txt\": [\"C\", \"CST\", \"CJT\"],\n",
    "    \"that_determiner.txt\": \"DT0\",\n",
    "    \"that_pronoun.txt\":  [\"ABC\", \"CST\", \"CJT\"]\n",
    "}\n",
    "\n",
    "# Initialiser TreeTagger\n",
    "assert os.path.exists(param_file), \"Le fichier de paramètres n'existe pas.\"\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGDIR=tagger_dir, TAGPARFILE=param_file)\n",
    "\n",
    "# Résultats globaux\n",
    "results = {}\n",
    "\n",
    "# Boucle sur chaque fichier de test\n",
    "for file_name, expected_tags in test_files.items():\n",
    "    input_file = os.path.expanduser(f\"~/treetagger/data/{file_name}\")  # Fichier d'entrée\n",
    "    output_file = os.path.expanduser(f\"~/treetagger/data/{file_name.replace('.txt', '-annotation.txt')}\")  # Fichier de sortie\n",
    "\n",
    "    assert os.path.exists(input_file), f\"Le fichier {input_file} n'existe pas.\"\n",
    "\n",
    "    # Lire le fichier de test\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().splitlines()\n",
    "\n",
    "    # Ouvrir le fichier de sortie\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        total_that = 0\n",
    "        correct_that = 0\n",
    "\n",
    "        # Appliquer TreeTagger et enregistrer les annotations\n",
    "        for line in text:\n",
    "            words = line.strip().split()\n",
    "            if words:\n",
    "                tags = tagger.tag_text(line)\n",
    "                for tagged_word in tags:\n",
    "                    parts = tagged_word.split(\"\\t\")\n",
    "                    if len(parts) == 3:\n",
    "                        word, predicted_tag, lemma = parts\n",
    "                        # Sauvegarder dans le fichier\n",
    "                        out_f.write(f\"{word}\\t{predicted_tag}\\t{lemma}\\n\")\n",
    "\n",
    "                        # Vérifier si \"that\" est bien annoté\n",
    "                        if word.lower() == \"that\":\n",
    "                            total_that += 1\n",
    "                            if isinstance(expected_tags, list):  # Si plusieurs tags sont possibles\n",
    "                                if predicted_tag in expected_tags:\n",
    "                                    correct_that += 1\n",
    "                            else:\n",
    "                                if predicted_tag == expected_tags:\n",
    "                                    correct_that += 1\n",
    "\n",
    "    # Calculer la précision sur \"that\"\n",
    "    accuracy = correct_that / total_that if total_that > 0 else 0\n",
    "    results[file_name] = {\"total_that\": total_that, \"correct\": correct_that, \"accuracy\": accuracy}\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"| Fichier                 | Précision | Total 'that' | Correctement annotés |\")\n",
    "print(\"|-------------------------|----------|-------------|---------------------|\")\n",
    "for file_name, data in results.items():\n",
    "    print(f\"| {file_name:<23} | {data['accuracy']:.2%} | {data['total_that']:<12} | {data['correct']:<20} |\")\n",
    "\n",
    "print(\"\\nLes annotations sont sauvegardées dans les fichiers correspondants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fichier                 | Précision | Total 'that' | Correctement annotés |\n",
      "|-------------------------|----------|-------------|---------------------|\n",
      "| that_adv.txt            | 0.00% | 100          | 0                    |\n",
      "| that_conjunction.txt    | 96.00% | 100          | 96                   |\n",
      "| that_determiner.txt     | 93.00% | 100          | 93                   |\n",
      "| that_pronoun.txt        | 23.00% | 100          | 23                   |\n",
      "\n",
      "Les annotations sont sauvegardées dans les fichiers correspondants.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import treetaggerwrapper\n",
    "\n",
    "# Définir les chemins\n",
    "tagger_dir = os.path.expanduser(\"~/treetagger\")  # Dossier TreeTagger\n",
    "param_file = \"english_brown_model.par\"  # Modèle anglais\n",
    "\n",
    "# Dictionnaire des fichiers de test et des étiquettes attendues\n",
    "test_files = {\n",
    "    \"that_adv.txt\":  \"RB\",\n",
    "    \"that_conjunction.txt\": [ \"CST\", \"CJT\"],\n",
    "    \"that_determiner.txt\": \"DT\",\n",
    "    \"that_pronoun.txt\":  \"WPR\"\n",
    "}\n",
    "\n",
    "# Initialiser TreeTagger\n",
    "assert os.path.exists(param_file), \"Le fichier de paramètres n'existe pas.\"\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGDIR=tagger_dir, TAGPARFILE=param_file)\n",
    "\n",
    "# Résultats globaux\n",
    "results = {}\n",
    "\n",
    "# Boucle sur chaque fichier de test\n",
    "for file_name, expected_tags in test_files.items():\n",
    "    input_file = os.path.expanduser(f\"~/treetagger/data/{file_name}\")  # Fichier d'entrée\n",
    "    output_file = os.path.expanduser(f\"~/treetagger/data/{file_name.replace('.txt', '-annotation.txt')}\")  # Fichier de sortie\n",
    "\n",
    "    assert os.path.exists(input_file), f\"Le fichier {input_file} n'existe pas.\"\n",
    "\n",
    "    # Lire le fichier de test\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().splitlines()\n",
    "\n",
    "    # Ouvrir le fichier de sortie\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        total_that = 0\n",
    "        correct_that = 0\n",
    "\n",
    "        # Appliquer TreeTagger et enregistrer les annotations\n",
    "        for line in text:\n",
    "            words = line.strip().split()\n",
    "            if words:\n",
    "                tags = tagger.tag_text(line)\n",
    "                for tagged_word in tags:\n",
    "                    parts = tagged_word.split(\"\\t\")\n",
    "                    if len(parts) == 3:\n",
    "                        word, predicted_tag, lemma = parts\n",
    "                        # Sauvegarder dans le fichier\n",
    "                        out_f.write(f\"{word}\\t{predicted_tag}\\t{lemma}\\n\")\n",
    "\n",
    "                        # Vérifier si \"that\" est bien annoté\n",
    "                        if word.lower() == \"that\":\n",
    "                            total_that += 1\n",
    "                            if isinstance(expected_tags, list):  # Si plusieurs tags sont possibles\n",
    "                                if predicted_tag in expected_tags:\n",
    "                                    correct_that += 1\n",
    "                            else:\n",
    "                                if predicted_tag == expected_tags:\n",
    "                                    correct_that += 1\n",
    "\n",
    "    # Calculer la précision sur \"that\"\n",
    "    accuracy = correct_that / total_that if total_that > 0 else 0\n",
    "    results[file_name] = {\"total_that\": total_that, \"correct\": correct_that, \"accuracy\": accuracy}\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"| Fichier                 | Précision | Total 'that' | Correctement annotés |\")\n",
    "print(\"|-------------------------|----------|-------------|---------------------|\")\n",
    "for file_name, data in results.items():\n",
    "    print(f\"| {file_name:<23} | {data['accuracy']:.2%} | {data['total_that']:<12} | {data['correct']:<20} |\")\n",
    "\n",
    "print(\"\\nLes annotations sont sauvegardées dans les fichiers correspondants.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/diamouserignetoubandiaye/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/diamouserignetoubandiaye/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus d'entraînement sauvegardé dans : corpus_brown_that_custom.txt\n",
      "Nombre total d'occurrences de 'that': 10457\n",
      "Nombre d'occurrences de 'that' pour chaque catégorie :\n",
      "CST: 3831\n",
      "CJT: 2636\n",
      "WPR: 1662\n",
      "DT: 2272\n",
      "RB: 56\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "brown_to_custom = {\n",
    "    'CS': 'CST',   # Potentially CST or CJT, to be determined by context\n",
    "    'DT': 'DT',    # Determiner\n",
    "    'WDT': 'WPR',  # Relative pronoun\n",
    "    'QL': 'RB',    # Adverb\n",
    "    'CS-HL': 'CST',\n",
    "    'CS-NC': 'CST',\n",
    "    'DT-HL': 'DT',\n",
    "    'DT-NC': 'DT',\n",
    "    'DT-TL': 'DT',\n",
    "    'WPS': 'WPR',\n",
    "    'WPS-HL': 'WPR',\n",
    "    'WPS-NC': 'WPR',\n",
    "    'WPS-TL': 'WPR'\n",
    "}\n",
    "\n",
    "def is_verb_tag(tag):\n",
    "    return tag.startswith('V') or tag in ['BE', 'HV', 'DO']\n",
    "\n",
    "def is_noun_tag(tag):\n",
    "    return tag.startswith('N') or tag in ['PPS', 'PPO']\n",
    "\n",
    "def prepare_brown_corpus_for_that(output_file):\n",
    "    training_data = []\n",
    "    category_counts = defaultdict(int)\n",
    "    total_that_count = 0\n",
    "\n",
    "    for sent in brown.tagged_sents():\n",
    "        sentence_tokens = []\n",
    "        contains_relevant_that = False\n",
    "        for i, (word, tag) in enumerate(sent):\n",
    "            lemma = lemmatizer.lemmatize(word.lower())\n",
    "            custom_tag = brown_to_custom.get(tag, tag)\n",
    "            \n",
    "            if custom_tag == 'CST':  # This includes 'CS', 'CS-HL', 'CS-NC'\n",
    "                prev_tag = sent[i-1][1] if i > 0 else ''\n",
    "                if is_verb_tag(prev_tag):\n",
    "                    custom_tag = 'CJT'\n",
    "                # If it's not a verb, it remains 'CST'\n",
    "            \n",
    "            if word.lower() == 'that' and custom_tag in ['WPR', 'CST', 'CJT', 'DT', 'RB']:\n",
    "                category_counts[custom_tag] += 1\n",
    "                total_that_count += 1\n",
    "                contains_relevant_that = True\n",
    "            \n",
    "            sentence_tokens.append(f\"{word}\\t{custom_tag}\\t{lemma}\")\n",
    "        \n",
    "        if contains_relevant_that:\n",
    "            training_data.extend(sentence_tokens)\n",
    "            training_data.append(\"\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"\\n\".join(training_data))\n",
    "\n",
    "    print(f\"Corpus d'entraînement sauvegardé dans : {output_file}\")\n",
    "    print(f\"Nombre total d'occurrences de 'that': {total_that_count}\")\n",
    "    print(\"Nombre d'occurrences de 'that' pour chaque catégorie :\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"{category}: {count}\")\n",
    "\n",
    "output_file = \"corpus_brown_that_custom.txt\"\n",
    "prepare_brown_corpus_for_that(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier lexicon créé : lexicon_brown.txt\n",
      "Fichier tag créé : tags_brown.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_lexicon_and_tag_files(input_file, lexicon_file, tag_file):\n",
    "    word_tag_lemma = defaultdict(set)\n",
    "    all_tags = set()\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            if line.strip():\n",
    "                word, tag, lemma = line.strip().split('\\t')\n",
    "                word_tag_lemma[word.lower()].add((tag, lemma))\n",
    "                all_tags.add(tag)\n",
    "    \n",
    "    # Écriture du fichier lexicon\n",
    "    with open(lexicon_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word, tag_lemmas in word_tag_lemma.items():\n",
    "            outfile.write(f\"{word}\\t\" + \"\\t\".join(f\"{tag} {lemma}\" for tag, lemma in tag_lemmas) + \"\\n\")\n",
    "    \n",
    "    # Écriture du fichier tag\n",
    "    with open(tag_file, 'w', encoding='utf-8') as outfile:\n",
    "        for tag in sorted(all_tags):\n",
    "            outfile.write(f\"{tag}\\n\")\n",
    "\n",
    "    print(f\"Fichier lexicon créé : {lexicon_file}\")\n",
    "    print(f\"Fichier tag créé : {tag_file}\")\n",
    "\n",
    "# Utilisation de la fonction\n",
    "input_file = \"corpus_brown_that_custom.txt\"\n",
    "lexicon_file = \"lexicon_brown.txt\"\n",
    "tag_file = \"tags_brown.txt\"\n",
    "\n",
    "create_lexicon_and_tag_files(input_file, lexicon_file, tag_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train-tree-tagger -cl 2 -dtg 1.00 -sw 1.00 -ecw 1.00 -stg 1.00 -ptg -1.00 lexicon_brown.txt tags_brown.txt corpus_brown_that_custom.txt english_brown_model.par\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "247000\tmaking affix tree ...\n",
      "prefix lexicon: 15945 nodes\n",
      "suffix lexicon: 3120 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "279449\t29083\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "58\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 59\n",
      "Max. path length: 24\n",
      "\n",
      "done.\n",
      "\n",
      "✅ Modèle entraîné avec succès : english_brown_model.par\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Définir les chemins\n",
    "tagger_bin = os.path.expanduser(\"~/treetagger/bin/train-tree-tagger\")  # Adapter selon l'OS\n",
    "lexicon_file = \"lexicon_brown.txt\"\n",
    "open_class_file = \"tags_brown.txt\"\n",
    "train_file = \"corpus_brown_that_custom.txt\"\n",
    "output_model = \"english_brown_model.par\"\n",
    "\n",
    "# Vérifier que les fichiers existent\n",
    "for file in [tagger_bin, lexicon_file, open_class_file, train_file]:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Erreur : Fichier introuvable -> {file}\")\n",
    "        exit(1)\n",
    "\n",
    "# Commande correcte pour entraîner TreeTagger\n",
    "train_command = [\n",
    "    tagger_bin, \"-utf8\",\n",
    "    \"-st\", \".\",\n",
    "    lexicon_file, open_class_file, train_file, output_model\n",
    "]\n",
    "\n",
    "# Exécuter l'entraînement\n",
    "process = subprocess.run(train_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Afficher la sortie\n",
    "print(process.stdout.decode())\n",
    "print(process.stderr.decode())\n",
    "\n",
    "# Vérifier si le fichier modèle a été généré\n",
    "if os.path.exists(output_model):\n",
    "    print(f\"✅ Modèle entraîné avec succès : {output_model}\")\n",
    "else:\n",
    "    print(f\"❌ Erreur : le modèle n'a pas été généré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Catégorie de 'that' | Précision Brown (english_brown_model.par) |\n",
      "|---------------------|-------------------------|\n",
      "| Adv                 | 0.00%                  |\n",
      "| Conjunction         | 96.00%                  |\n",
      "| Determiner          | 93.00%                  |\n",
      "| Pronoun             | 23.00%                  |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import treetaggerwrapper\n",
    "\n",
    "# Définir le chemin vers TreeTagger\n",
    "tagger_dir = os.path.expanduser(\"~/treetagger\")\n",
    "os.environ['TAGDIR'] = tagger_dir\n",
    "\n",
    "\n",
    "# Initialiser les taggers\n",
    "\n",
    "tagger_brown = treetaggerwrapper.TreeTagger(TAGLANG='en',TAGDIR=tagger_dir, TAGPARFILE=\"english_brown_model.par\")\n",
    "\n",
    "\n",
    "# Fonction pour calculer la précision\n",
    "#Cette ligne définit la fonction. Elle prend deux paramètres :\n",
    "\n",
    "    #tagged_sentences : Une liste de phrases étiquetées par TreeTagger. Chaque phrase est une liste de tuples (mot, tag, lemme).\n",
    "    #expected_tags : Une liste des tags corrects attendus pour 'that' dans cette catégorie grammaticale.\n",
    "\n",
    "def calculate_precision(tagged_sentences, expected_tags):\n",
    "    correct = sum(1 for sent in tagged_sentences for word, tag, _ in sent\n",
    "                  if word.lower() == 'that' and tag in expected_tags)\n",
    "    total = sum(1 for sent in tagged_sentences for word, _, _ in sent\n",
    "                if word.lower() == 'that')\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Fichiers de test et leurs catégories correspondantes\n",
    "test_files = {\n",
    "    os.path.expanduser(\"~/treetagger/data/that_adv.txt\"): 'RB',\n",
    "    os.path.expanduser(\"~/treetagger/data/that_conjunction.txt\"): ['CST','CJT'],\n",
    "    os.path.expanduser(\"~/treetagger/data/that_determiner.txt\"): 'DT',\n",
    "    os.path.expanduser(\"~/treetagger/data/that_pronoun.txt\"): 'WPR'\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for file, tags in test_files.items():\n",
    "    with open(file, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    \n",
    "    tagged_brown = [tagger_brown.tag_text(sent) for sent in sentences]\n",
    "    parsed_brown = [treetaggerwrapper.make_tags(tag) for tag in tagged_brown]\n",
    "\n",
    "\n",
    "    # Calculer les précisions\n",
    "    precision_brown = calculate_precision(parsed_brown, tags)\n",
    "\n",
    "    results[file] = {'Brown': precision_brown}\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"| Catégorie de 'that' | Précision Brown (english_brown_model.par) |\")\n",
    "print(\"|---------------------|-------------------------|\")\n",
    "for file, precisions in results.items():\n",
    "    category = file.split('.')[0].split('_')[1].capitalize()\n",
    "    print(f\"| {category:<19} | {precisions['Brown']:.2%}{' ':>17} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Tag | Nombre d'occurrences |\n",
      "|-----|----------------------|\n",
      "| AT   | 22526                |\n",
      "| NP-TL | 731                  |\n",
      "| NN-TL | 2543                 |\n",
      "| JJ-TL | 824                  |\n",
      "| VBD  | 5558                 |\n",
      "| NR   | 263                  |\n",
      "| NN   | 34845                |\n",
      "| IN   | 27526                |\n",
      "| NP$  | 536                  |\n",
      "| JJ   | 15393                |\n",
      "| ``   | 1677                 |\n",
      "| ''   | 1625                 |\n",
      "| CST  | 7437                 |\n",
      "| DTI  | 803                  |\n",
      "| NNS  | 11753                |\n",
      "| .    | 9758                 |\n",
      "| RBR  | 286                  |\n",
      "| ,    | 12522                |\n",
      "| WPR  | 3458                 |\n",
      "| HVD  | 1387                 |\n",
      "| VBZ  | 2211                 |\n",
      "| CC   | 8268                 |\n",
      "| IN-TL | 300                  |\n",
      "| BEDZ | 2644                 |\n",
      "| VBN  | 7113                 |\n",
      "| PPS  | 5038                 |\n",
      "| DOD  | 281                  |\n",
      "| VB   | 8636                 |\n",
      "| CJT  | 3050                 |\n",
      "| AP   | 2347                 |\n",
      "| BER  | 1048                 |\n",
      "| RB   | 11057                |\n",
      "| NP   | 6515                 |\n",
      "| TO   | 3668                 |\n",
      "| HV   | 1035                 |\n",
      "| DTS  | 600                  |\n",
      "| VBG  | 3838                 |\n",
      "| PPO  | 2476                 |\n",
      "| DT   | 3761                 |\n",
      "| MD   | 3931                 |\n",
      "| BE   | 1943                 |\n",
      "| PPSS | 3429                 |\n",
      "| WRB  | 1018                 |\n",
      "| CD   | 2344                 |\n",
      "| *    | 1438                 |\n",
      "| EX   | 564                  |\n",
      "| :    | 292                  |\n",
      "| (    | 441                  |\n",
      "| )    | 438                  |\n",
      "| HVZ  | 694                  |\n",
      "| BEN  | 639                  |\n",
      "| RP   | 1137                 |\n",
      "| JJT  | 250                  |\n",
      "| NPS  | 338                  |\n",
      "| BEZ  | 3195                 |\n",
      "| OD   | 392                  |\n",
      "| AT-TL | 160                  |\n",
      "| BEG  | 170                  |\n",
      "| BED  | 748                  |\n",
      "| PP$  | 3805                 |\n",
      "| PN   | 736                  |\n",
      "| DOZ  | 146                  |\n",
      "| PPL  | 324                  |\n",
      "| JJR  | 481                  |\n",
      "| MD*  | 188                  |\n",
      "| NNS-TL | 496                  |\n",
      "| ABN  | 747                  |\n",
      "| RBT  | 27                   |\n",
      "| VBG-TL | 21                   |\n",
      "| NNS$ | 47                   |\n",
      "| FW-IN | 14                   |\n",
      "| FW-DT | 2                    |\n",
      "| VBN-TL | 158                  |\n",
      "| NR-TL | 97                   |\n",
      "| NNS$-TL | 18                   |\n",
      "| NN$  | 277                  |\n",
      "| ABL  | 137                  |\n",
      "| DOD* | 89                   |\n",
      "| ABX  | 156                  |\n",
      "| WPO  | 62                   |\n",
      "| HVG  | 72                   |\n",
      "| DO   | 358                  |\n",
      "| --   | 847                  |\n",
      "| DTX  | 26                   |\n",
      "| NN$-TL | 85                   |\n",
      "| AP-TL | 6                    |\n",
      "| UH-TL | 4                    |\n",
      "| DOZ* | 24                   |\n",
      "| PPLS | 103                  |\n",
      "| BEM  | 78                   |\n",
      "| PP$$ | 45                   |\n",
      "| PPS+BEZ | 63                   |\n",
      "| JJS  | 69                   |\n",
      "| PPSS+MD | 60                   |\n",
      "| NP$-TL | 22                   |\n",
      "| PPSS+HV | 25                   |\n",
      "| NPS$ | 7                    |\n",
      "| HVN  | 62                   |\n",
      "| '    | 78                   |\n",
      "| HVD* | 21                   |\n",
      "| UH   | 68                   |\n",
      "| BEZ* | 32                   |\n",
      "| OD-TL | 51                   |\n",
      "| WP$  | 54                   |\n",
      "| CD-TL | 112                  |\n",
      "| NR$  | 8                    |\n",
      "| PPSS+BER | 36                   |\n",
      "| DT+BEZ | 6                    |\n",
      "| DO*  | 71                   |\n",
      "| CC-TL | 44                   |\n",
      "| BEDZ* | 45                   |\n",
      "| FW-JJ | 17                   |\n",
      "| FW-NN | 62                   |\n",
      "| BER* | 7                    |\n",
      "| RB+BEZ | 1                    |\n",
      "| PPSS+BEM | 40                   |\n",
      "| NPS-TL | 7                    |\n",
      "| FW-NN-TL | 28                   |\n",
      "| FW-JJ-TL | 9                    |\n",
      "| FW-AT-TL | 9                    |\n",
      "| HVZ* | 3                    |\n",
      "| RN   | 2                    |\n",
      "| PPS+MD | 25                   |\n",
      "| RB$  | 3                    |\n",
      "| VBZ-TL | 4                    |\n",
      "| WPO-TL | 1                    |\n",
      "| NR$-TL | 1                    |\n",
      "| IN-HL | 7                    |\n",
      "| JJ-HL | 2                    |\n",
      "| NNS-HL | 5                    |\n",
      "| .-HL | 5                    |\n",
      "| PN$  | 25                   |\n",
      "| AT-HL | 1                    |\n",
      "| NN-HL | 5                    |\n",
      "| EX+BEZ | 18                   |\n",
      "| QLP  | 70                   |\n",
      "| VB-TL | 13                   |\n",
      "| AP$  | 3                    |\n",
      "| FW-NNS | 16                   |\n",
      "| FW-WDT | 4                    |\n",
      "| PPSS-TL | 3                    |\n",
      "| MD-TL | 1                    |\n",
      "| FW-AT | 5                    |\n",
      "| FW-VBZ | 1                    |\n",
      "| FW-PPO | 1                    |\n",
      "| ABN-TL | 3                    |\n",
      "| BED* | 4                    |\n",
      "| WRB-TL | 2                    |\n",
      "| BER-TL | 1                    |\n",
      "| NN-NC | 28                   |\n",
      "| WPS+BEZ | 4                    |\n",
      "| VBD-TL | 3                    |\n",
      "| RB-TL | 11                   |\n",
      "| DO-TL | 2                    |\n",
      "| PPO-TL | 2                    |\n",
      "| PP$-TL | 7                    |\n",
      "| PN-TL | 1                    |\n",
      "| FW-CD | 3                    |\n",
      "| WQL  | 45                   |\n",
      "| HVZ-TL | 2                    |\n",
      "| FW-JJ-NC | 1                    |\n",
      "| FW-NN$ | 3                    |\n",
      "| JJR-TL | 1                    |\n",
      "| :-TL | 3                    |\n",
      "| VB+PPO | 7                    |\n",
      "| FW-IN+AT-TL | 2                    |\n",
      "| FW-NNS-TL | 8                    |\n",
      "| FW-CC | 6                    |\n",
      "| WDT+BEZ | 4                    |\n",
      "| FW-RB | 9                    |\n",
      "| FW-VBN | 6                    |\n",
      "| JJS-TL | 2                    |\n",
      "| HV*  | 2                    |\n",
      "| WPS+MD | 1                    |\n",
      "| NP-NC | 1                    |\n",
      "| PPS+HVD | 20                   |\n",
      "| FW-BEZ | 1                    |\n",
      "| FW-BER | 2                    |\n",
      "| FW-IN-TL | 5                    |\n",
      "| FW-AT+NN-TL | 2                    |\n",
      "| FW-VB | 4                    |\n",
      "| FW-VBG | 3                    |\n",
      "| FW-PPL | 4                    |\n",
      "| DT$  | 1                    |\n",
      "| FW-PPO+IN | 1                    |\n",
      "| IN-NC | 18                   |\n",
      "| FW-UH | 1                    |\n",
      "| JJT-TL | 2                    |\n",
      "| FW-CC-TL | 1                    |\n",
      "| FW-CD-TL | 1                    |\n",
      "| NIL  | 9                    |\n",
      "| TO-TL | 1                    |\n",
      "| BEZ-TL | 1                    |\n",
      "| NN-TL-HL | 1                    |\n",
      "| CD-TL-HL | 1                    |\n",
      "| (-HL | 5                    |\n",
      "| CD-HL | 4                    |\n",
      "| )-HL | 5                    |\n",
      "| FW-NN$-TL | 1                    |\n",
      "| NP-HL | 2                    |\n",
      "| JJ-NC | 16                   |\n",
      "| NRS  | 2                    |\n",
      "| VB-NC | 13                   |\n",
      "| PPO-NC | 2                    |\n",
      "| PPSS+BER-N | 1                    |\n",
      "| UH-NC | 4                    |\n",
      "| HVZ-NC | 1                    |\n",
      "| VBN-NC | 2                    |\n",
      "| WDT-NC | 4                    |\n",
      "| BEDZ-NC | 7                    |\n",
      "| .-NC | 4                    |\n",
      "| EX-NC | 1                    |\n",
      "| BER*-NC | 1                    |\n",
      "| AP-NC | 1                    |\n",
      "| NNS-NC | 8                    |\n",
      "| AT-NC | 11                   |\n",
      "| PPSS-NC | 12                   |\n",
      "| VBD-NC | 7                    |\n",
      "| WRB-NC | 2                    |\n",
      "| BED-NC | 3                    |\n",
      "| CC-NC | 1                    |\n",
      "| RB-NC | 2                    |\n",
      "| PPS+BEZ-NC | 1                    |\n",
      "| MD-NC | 1                    |\n",
      "| PPSS+BER-NC | 1                    |\n",
      "| VBG-NC | 3                    |\n",
      "| PP$-NC | 4                    |\n",
      "| ABN-NC | 1                    |\n",
      "| ,-NC | 1                    |\n",
      "| JJT-NC | 1                    |\n",
      "| WPO-NC | 1                    |\n",
      "| TO-NC | 3                    |\n",
      "| HV-NC | 3                    |\n",
      "| BEM-NC | 1                    |\n",
      "| NPS-NC | 1                    |\n",
      "| RP-NC | 1                    |\n",
      "| VB-HL | 3                    |\n",
      "| VBG-HL | 2                    |\n",
      "| CC-HL | 1                    |\n",
      "| FW-QL | 1                    |\n",
      "| FW-CS | 1                    |\n",
      "| WPS+BEZ-TL | 1                    |\n",
      "| WPS+HVZ | 1                    |\n",
      "| VB+IN | 1                    |\n",
      "| MD+PPSS | 1                    |\n",
      "| PPSS+HVD | 14                   |\n",
      "| NN+HVD-TL | 1                    |\n",
      "| EX+MD | 1                    |\n",
      "| NN+BEZ | 4                    |\n",
      "| NP+BEZ | 6                    |\n",
      "| FW-NP | 1                    |\n",
      "| PPS+HVZ | 3                    |\n",
      "| EX+HVD | 1                    |\n",
      "| MD+HV | 2                    |\n",
      "| WRB+BEZ | 1                    |\n",
      "| FW-NP-TL | 1                    |\n",
      "| VBG+TO | 4                    |\n",
      "| PPSS+VB | 1                    |\n",
      "| FW-IN+AT | 1                    |\n",
      "| WPS+HVD | 1                    |\n",
      "| PN+HVD | 1                    |\n",
      "| FW-VB-NC | 1                    |\n",
      "| FW-NN-NC | 1                    |\n",
      "| FW-IN+NN | 1                    |\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Chemin du corpus d'entraînement\n",
    "corpus_file = \"corpus_brown_that_custom.txt\"\n",
    "\n",
    "# Lire et analyser le corpus\n",
    "tag_counts = Counter()\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if \"\\t\" in line:\n",
    "            _, tag, _ = line.strip().split(\"\\t\")\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"| Tag | Nombre d'occurrences |\")\n",
    "print(\"|-----|----------------------|\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"| {tag:<4} | {count:<20} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Tag  | Nombre d'occurrences |\n",
      "|------|----------------------|\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Définir le chemin du fichier\n",
    "corpus_file = os.path.expanduser(\"~/treetagger/data/that_adv.txt\")  # Assurez-vous que ce fichier est correct\n",
    "\n",
    "# Vérifier si le fichier existe\n",
    "if not os.path.exists(corpus_file):\n",
    "    print(f\"Erreur : Le fichier {corpus_file} n'existe pas. Vérifiez le chemin.\")\n",
    "else:\n",
    "    # Initialiser un compteur pour les tags\n",
    "    tag_counts = Counter()\n",
    "\n",
    "    # Lire et analyser le corpus\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if \"\\t\" in line:  # Vérifier que la ligne contient bien une annotation\n",
    "                _, tag, _ = line.strip().split(\"\\t\")\n",
    "                tag_counts[tag] += 1\n",
    "\n",
    "    # Afficher les statistiques\n",
    "    print(\"\\n| Tag  | Nombre d'occurrences |\")\n",
    "    print(\"|------|----------------------|\")\n",
    "    for tag, count in tag_counts.items():\n",
    "        print(f\"| {tag:<5} | {count:<20} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Tag | Nombre d'occurrences |\n",
      "|-----|----------------------|\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Chemin du corpus d'entraînement\n",
    "corpus_file = os.path.expanduser(\"~/treetagger/data/that_adv.txt\")\n",
    "\n",
    "# Lire et analyser le corpus\n",
    "tag_counts = Counter()\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if \"\\t\" in line:\n",
    "            _, tag, _ = line.strip().split(\"\\t\")\n",
    "            tag_counts[tag] += 1\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(\"| Tag | Nombre d'occurrences |\")\n",
    "print(\"|-----|----------------------|\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"| {tag:<4} | {count:<20} |\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
