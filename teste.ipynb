{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus d'entraînement sauvegardé dans : corpus_brown_that_custom.txt\n",
      "Nombre total d'occurrences de 'that': 10457\n",
      "Nombre d'occurrences de 'that' pour chaque catégorie :\n",
      "CST: 3831\n",
      "CJT: 2636\n",
      "WPR: 1662\n",
      "DT: 2272\n",
      "RB: 56\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "brown_to_custom = {\n",
    "    'CS': 'CST',   # Potentially CST or CJT, to be determined by context\n",
    "    'DT': 'DT',    # Determiner\n",
    "    'WDT': 'WPR',  # Relative pronoun\n",
    "    'QL': 'RB',    # Adverb\n",
    "    'CS-HL': 'CST',\n",
    "    'CS-NC': 'CST',\n",
    "    'DT-HL': 'DT',\n",
    "    'DT-NC': 'DT',\n",
    "    'DT-TL': 'DT',\n",
    "    'WPS': 'WPR',\n",
    "    'WPS-HL': 'WPR',\n",
    "    'WPS-NC': 'WPR',\n",
    "    'WPS-TL': 'WPR'\n",
    "}\n",
    "\n",
    "def is_verb_tag(tag):\n",
    "    return tag.startswith('V') or tag in ['BE', 'HV', 'DO']\n",
    "\n",
    "def is_noun_tag(tag):\n",
    "    return tag.startswith('N') or tag in ['PPS', 'PPO']\n",
    "\n",
    "def prepare_brown_corpus_for_that(output_file):\n",
    "    training_data = []\n",
    "    category_counts = defaultdict(int)\n",
    "    total_that_count = 0\n",
    "\n",
    "    for sent in brown.tagged_sents():\n",
    "        sentence_tokens = []\n",
    "        contains_relevant_that = False\n",
    "        for i, (word, tag) in enumerate(sent):\n",
    "            lemma = lemmatizer.lemmatize(word.lower())\n",
    "            custom_tag = brown_to_custom.get(tag, tag)\n",
    "            \n",
    "            if custom_tag == 'CST':  # This includes 'CS', 'CS-HL', 'CS-NC'\n",
    "                prev_tag = sent[i-1][1] if i > 0 else ''\n",
    "                if is_verb_tag(prev_tag):\n",
    "                    custom_tag = 'CJT'\n",
    "                # If it's not a verb, it remains 'CST'\n",
    "            \n",
    "            if word.lower() == 'that' and custom_tag in ['WPR', 'CST', 'CJT', 'DT', 'RB']:\n",
    "                category_counts[custom_tag] += 1\n",
    "                total_that_count += 1\n",
    "                contains_relevant_that = True\n",
    "            \n",
    "            sentence_tokens.append(f\"{word}\\t{custom_tag}\\t{lemma}\")\n",
    "        \n",
    "        if contains_relevant_that:\n",
    "            training_data.extend(sentence_tokens)\n",
    "            training_data.append(\"\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"\\n\".join(training_data))\n",
    "\n",
    "    print(f\"Corpus d'entraînement sauvegardé dans : {output_file}\")\n",
    "    print(f\"Nombre total d'occurrences de 'that': {total_that_count}\")\n",
    "    print(\"Nombre d'occurrences de 'that' pour chaque catégorie :\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"{category}: {count}\")\n",
    "\n",
    "output_file = \"corpus_brown_that_custom.txt\"\n",
    "prepare_brown_corpus_for_that(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier lexicon créé : lexicon_brown.txt\n",
      "Fichier tag créé : tags_brown.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_lexicon_and_tag_files(input_file, lexicon_file, tag_file):\n",
    "    word_tag_lemma = defaultdict(set)\n",
    "    all_tags = set()\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            if line.strip():\n",
    "                word, tag, lemma = line.strip().split('\\t')\n",
    "                word_tag_lemma[word.lower()].add((tag, lemma))\n",
    "                all_tags.add(tag)\n",
    "    \n",
    "    # Écriture du fichier lexicon\n",
    "    with open(lexicon_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word, tag_lemmas in word_tag_lemma.items():\n",
    "            outfile.write(f\"{word}\\t\" + \"\\t\".join(f\"{tag} {lemma}\" for tag, lemma in tag_lemmas) + \"\\n\")\n",
    "    \n",
    "    # Écriture du fichier tag\n",
    "    with open(tag_file, 'w', encoding='utf-8') as outfile:\n",
    "        for tag in sorted(all_tags):\n",
    "            outfile.write(f\"{tag}\\n\")\n",
    "\n",
    "    print(f\"Fichier lexicon créé : {lexicon_file}\")\n",
    "    print(f\"Fichier tag créé : {tag_file}\")\n",
    "\n",
    "# Utilisation de la fonction\n",
    "input_file = \"corpus_brown_that_custom.txt\"\n",
    "lexicon_file = \"lexicon_brown.txt\"\n",
    "tag_file = \"tags_brown.txt\"\n",
    "\n",
    "create_lexicon_and_tag_files(input_file, lexicon_file, tag_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "train-tree-tagger -cl 2 -dtg 1.00 -sw 1.00 -ecw 1.00 -stg 1.00 -ptg -1.00 lexicon_brown.txt tags_brown.txt corpus_brown_that_custom.txt english_brown_model.par\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "247000\tmaking affix tree ...\n",
      "prefix lexicon: 15945 nodes\n",
      "suffix lexicon: 3120 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "279449\t29083\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "58\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 59\n",
      "Max. path length: 24\n",
      "\n",
      "done.\n",
      "\n",
      "✅ Modèle entraîné avec succès : english_brown_model.par\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Définir les chemins\n",
    "tagger_bin = os.path.expanduser(\"~/treetagger/bin/train-tree-tagger\")  # Adapter selon l'OS\n",
    "lexicon_file = \"lexicon_brown.txt\"\n",
    "open_class_file = \"tags_brown.txt\"\n",
    "train_file = \"corpus_brown_that_custom.txt\"\n",
    "output_model = \"english_brown_model.par\"\n",
    "\n",
    "# Vérifier que les fichiers existent\n",
    "for file in [tagger_bin, lexicon_file, open_class_file, train_file]:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"Erreur : Fichier introuvable -> {file}\")\n",
    "        exit(1)\n",
    "\n",
    "# Commande correcte pour entraîner TreeTagger\n",
    "train_command = [\n",
    "    tagger_bin, \"-utf8\",\n",
    "    \"-st\", \".\",\n",
    "    lexicon_file, open_class_file, train_file, output_model\n",
    "]\n",
    "\n",
    "# Exécuter l'entraînement\n",
    "process = subprocess.run(train_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Afficher la sortie\n",
    "print(process.stdout.decode())\n",
    "print(process.stderr.decode())\n",
    "\n",
    "# Vérifier si le fichier modèle a été généré\n",
    "if os.path.exists(output_model):\n",
    "    print(f\"✅ Modèle entraîné avec succès : {output_model}\")\n",
    "else:\n",
    "    print(f\"❌ Erreur : le modèle n'a pas été généré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TreeTaggerError",
     "evalue": "Bad TreeTagger directory: /Users/diamouserignetoubandiaye/treetagger/bin/train-tree-tagger",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTreeTaggerError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAGDIR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tagger_dir\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialiser les taggers\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tagger_brown \u001b[38;5;241m=\u001b[39m treetaggerwrapper\u001b[38;5;241m.\u001b[39mTreeTagger(TAGLANG\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m,TAGDIR\u001b[38;5;241m=\u001b[39mtagger_dir, TAGPARFILE\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish_brown_model.par\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Fonction pour calculer la précision\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Cette ligne définit la fonction. Elle prend deux paramètres :\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#tagged_sentences : Une liste de phrases étiquetées par TreeTagger. Chaque phrase est une liste de tuples (mot, tag, lemme).\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#expected_tags : Une liste des tags corrects attendus pour 'that' dans cette catégorie grammaticale.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_precision\u001b[39m(tagged_sentences, expected_tags):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/treetaggerwrapper.py:1006\u001b[0m, in \u001b[0;36mTreeTagger.__init__\u001b[0;34m(self, **kargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing treetaggerwrapper.py from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, osp\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18m__file__\u001b[39m))\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_language(kargs)\n\u001b[0;32m-> 1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_tagger(kargs)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_preprocessor(kargs)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# Note: TreeTagger process is started later, when really needed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/treetaggerwrapper.py:1053\u001b[0m, in \u001b[0;36mTreeTagger._set_tagger\u001b[0;34m(self, kargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdir):\n\u001b[1;32m   1052\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad TreeTagger directory: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdir)\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TreeTaggerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad TreeTagger directory: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdir)\n\u001b[1;32m   1054\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagdir=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdir)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# ----- Set subdirectories.\u001b[39;00m\n",
      "\u001b[0;31mTreeTaggerError\u001b[0m: Bad TreeTagger directory: /Users/diamouserignetoubandiaye/treetagger/bin/train-tree-tagger"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import treetaggerwrapper\n",
    "\n",
    "# Définir le chemin vers TreeTagger\n",
    "tagger_dir = os.path.expanduser(\"~/treetagger/bin/train-tree-tagger\")  # Remplacez par le chemin réel de votre installation TreeTagger\n",
    "os.environ['TAGDIR'] = tagger_dir\n",
    "\n",
    "\n",
    "# Initialiser les taggers\n",
    "\n",
    "tagger_brown = treetaggerwrapper.TreeTagger(TAGLANG='en',TAGDIR=tagger_dir, TAGPARFILE=\"english_brown_model.par\")\n",
    "\n",
    "\n",
    "# Fonction pour calculer la précision\n",
    "#Cette ligne définit la fonction. Elle prend deux paramètres :\n",
    "\n",
    "    #tagged_sentences : Une liste de phrases étiquetées par TreeTagger. Chaque phrase est une liste de tuples (mot, tag, lemme).\n",
    "    #expected_tags : Une liste des tags corrects attendus pour 'that' dans cette catégorie grammaticale.\n",
    "\n",
    "def calculate_precision(tagged_sentences, expected_tags):\n",
    "    correct = sum(1 for sent in tagged_sentences for word, tag, _ in sent\n",
    "                  if word.lower() == 'that' and tag in expected_tags)\n",
    "    total = sum(1 for sent in tagged_sentences for word, _, _ in sent\n",
    "                if word.lower() == 'that')\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Fichiers de test et leurs catégories correspondantes\n",
    "test_files = {\n",
    "    'that_adv.txt': 'RB',\n",
    "    'that_conjunction.txt': ['CST','CJT'],\n",
    "    'that_determiner.txt': 'DT',\n",
    "    'that_pronoun.txt': 'WPR'\n",
    "}\n",
    "results = {}\n",
    "\n",
    "for file, tags in test_files.items():\n",
    "    with open(file, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    \n",
    "    tagged_brown = [tagger_brown.tag_text(sent) for sent in sentences]\n",
    "    parsed_brown = [treetaggerwrapper.make_tags(tag) for tag in tagged_brown]\n",
    "\n",
    "\n",
    "    # Calculer les précisions\n",
    "    precision_brown = calculate_precision(parsed_brown, tags)\n",
    "\n",
    "    results[file] = {'Brown': precision_brown}\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"| Catégorie de 'that' | Précision Brown (english_brown_model.par) |\")\n",
    "print(\"|---------------------|-------------------------|\")\n",
    "for file, precisions in results.items():\n",
    "    category = file.split('.')[0].split('_')[1].capitalize()\n",
    "    print(f\"| {category:<19} | {precisions['Brown']:.2%}{' ':>17} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
