{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus d'entraînement sauvegardé dans : corpus_brown_that_custom.txt\n",
      "Nombre total d'occurrences de 'that': 10457\n",
      "Nombre d'occurrences de 'that' pour chaque catégorie :\n",
      "CST: 3831\n",
      "CJT: 2636\n",
      "WPR: 1662\n",
      "DT: 2272\n",
      "RB: 56\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "brown_to_custom = {\n",
    "    'CS': 'CST',   # Potentially CST or CJT, to be determined by context\n",
    "    'DT': 'DT',    # Determiner\n",
    "    'WDT': 'WPR',  # Relative pronoun\n",
    "    'QL': 'RB',    # Adverb\n",
    "    'CS-HL': 'CST',\n",
    "    'CS-NC': 'CST',\n",
    "    'DT-HL': 'DT',\n",
    "    'DT-NC': 'DT',\n",
    "    'DT-TL': 'DT',\n",
    "    'WPS': 'WPR',\n",
    "    'WPS-HL': 'WPR',\n",
    "    'WPS-NC': 'WPR',\n",
    "    'WPS-TL': 'WPR'\n",
    "}\n",
    "\n",
    "def is_verb_tag(tag):\n",
    "    return tag.startswith('V') or tag in ['BE', 'HV', 'DO']\n",
    "\n",
    "def is_noun_tag(tag):\n",
    "    return tag.startswith('N') or tag in ['PPS', 'PPO']\n",
    "\n",
    "def prepare_brown_corpus_for_that(output_file):\n",
    "    training_data = []\n",
    "    category_counts = defaultdict(int)\n",
    "    total_that_count = 0\n",
    "\n",
    "    for sent in brown.tagged_sents():\n",
    "        sentence_tokens = []\n",
    "        contains_relevant_that = False\n",
    "        for i, (word, tag) in enumerate(sent):\n",
    "            lemma = lemmatizer.lemmatize(word.lower())\n",
    "            custom_tag = brown_to_custom.get(tag, tag)\n",
    "            \n",
    "            if custom_tag == 'CST':  # This includes 'CS', 'CS-HL', 'CS-NC'\n",
    "                prev_tag = sent[i-1][1] if i > 0 else ''\n",
    "                if is_verb_tag(prev_tag):\n",
    "                    custom_tag = 'CJT'\n",
    "                # If it's not a verb, it remains 'CST'\n",
    "            \n",
    "            if word.lower() == 'that' and custom_tag in ['WPR', 'CST', 'CJT', 'DT', 'RB']:\n",
    "                category_counts[custom_tag] += 1\n",
    "                total_that_count += 1\n",
    "                contains_relevant_that = True\n",
    "            \n",
    "            sentence_tokens.append(f\"{word}\\t{custom_tag}\\t{lemma}\")\n",
    "        \n",
    "        if contains_relevant_that:\n",
    "            training_data.extend(sentence_tokens)\n",
    "            training_data.append(\"\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"\\n\".join(training_data))\n",
    "\n",
    "    print(f\"Corpus d'entraînement sauvegardé dans : {output_file}\")\n",
    "    print(f\"Nombre total d'occurrences de 'that': {total_that_count}\")\n",
    "    print(\"Nombre d'occurrences de 'that' pour chaque catégorie :\")\n",
    "    for category, count in category_counts.items():\n",
    "        print(f\"{category}: {count}\")\n",
    "\n",
    "output_file = \"corpus_brown_that_custom.txt\"\n",
    "prepare_brown_corpus_for_that(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier lexicon créé : lexicon_brown.txt\n",
      "Fichier tag créé : tags_brown.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def create_lexicon_and_tag_files(input_file, lexicon_file, tag_file):\n",
    "    word_tag_lemma = defaultdict(set)\n",
    "    all_tags = set()\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            if line.strip():\n",
    "                word, tag, lemma = line.strip().split('\\t')\n",
    "                word_tag_lemma[word.lower()].add((tag, lemma))\n",
    "                all_tags.add(tag)\n",
    "    \n",
    "    # Écriture du fichier lexicon\n",
    "    with open(lexicon_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word, tag_lemmas in word_tag_lemma.items():\n",
    "            outfile.write(f\"{word}\\t\" + \"\\t\".join(f\"{tag} {lemma}\" for tag, lemma in tag_lemmas) + \"\\n\")\n",
    "    \n",
    "    # Écriture du fichier tag\n",
    "    with open(tag_file, 'w', encoding='utf-8') as outfile:\n",
    "        for tag in sorted(all_tags):\n",
    "            outfile.write(f\"{tag}\\n\")\n",
    "\n",
    "    print(f\"Fichier lexicon créé : {lexicon_file}\")\n",
    "    print(f\"Fichier tag créé : {tag_file}\")\n",
    "\n",
    "# Utilisation de la fonction\n",
    "input_file = \"corpus_brown_that_custom.txt\"\n",
    "lexicon_file = \"lexicon_brown.txt\"\n",
    "tag_file = \"tags_brown.txt\"\n",
    "\n",
    "create_lexicon_and_tag_files(input_file, lexicon_file, tag_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
